"""
è³‡å®‰æ–°èæ”¶é›†å™¨ - æ ¸å¿ƒæ¨¡çµ„
============================
è‡ªå‹•æ”¶é›†å¤šå€‹è³‡å®‰æ–°èä¾†æºçš„ RSS Feedï¼Œ
æä¾›åˆ†é¡ã€æœå°‹ã€ç¯©é¸åŠŸèƒ½ï¼Œä¸¦å¯åŒ¯å‡º Excel/CSV/JSONã€‚
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import pandas as pd
import hashlib
import re
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed


@dataclass
class NewsItem:
    """æ–°èé …ç›®è³‡æ–™çµæ§‹"""
    title: str
    link: str
    date: str
    summary: str
    source: str
    category: str = ""
    keywords: List[str] = field(default_factory=list)
    content_hash: str = ""


class SecurityNewsCollector:
    """
    è³‡å®‰æ–°èæ”¶é›†å™¨
    
    åŠŸèƒ½ï¼š
    - å¾å¤šå€‹ RSS ä¾†æºæ”¶é›†æ–°è
    - è‡ªå‹•åˆ†é¡ï¼ˆæƒ¡æ„ç¨‹å¼ã€æ¼æ´ã€è³‡æ–™å¤–æ´©ç­‰ï¼‰
    - é—œéµå­—æ“·å–
    - æœå°‹èˆ‡ç¯©é¸
    - åŒ¯å‡º Excel/CSV/JSON
    """
    
    # é è¨­æ–°èä¾†æº
    DEFAULT_SOURCES = {
        # å°ç£ä¾†æº
        'iThome è³‡å®‰': 'https://www.ithome.com.tw/rss/security',
        'TWCERT/CC': 'https://www.twcert.org.tw/rss',
        
        # åœ‹éš›ä¾†æº
        'The Hacker News': 'https://feeds.feedburner.com/TheHackersNews',
        'Krebs on Security': 'https://krebsonsecurity.com/feed/',
        'BleepingComputer': 'https://www.bleepingcomputer.com/feed/',
        'Dark Reading': 'https://www.darkreading.com/rss.xml',
        'SecurityWeek': 'https://feeds.feedburner.com/securityweek',
        'Threatpost': 'https://threatpost.com/feed/',
        'HackRead': 'https://www.hackread.com/feed/',
        'Sophos News': 'https://news.sophos.com/en-us/feed/',
    }
    
    # åˆ†é¡é—œéµå­—
    CATEGORY_KEYWORDS = {
        'æƒ¡æ„ç¨‹å¼/Malware': ['malware', 'ransomware', 'trojan', 'virus', 'worm', 'botnet', 
                           'æƒ¡æ„ç¨‹å¼', 'å‹’ç´¢è»Ÿé«”', 'æœ¨é¦¬', 'ç—…æ¯’', 'è •èŸ²', 'æ®­å±ç¶²è·¯'],
        'æ¼æ´/Vulnerability': ['vulnerability', 'cve', 'exploit', 'zero-day', '0day', 'patch',
                              'æ¼æ´', 'å¼±é»', 'ä¿®è£œ', 'é›¶æ—¥'],
        'è³‡æ–™å¤–æ´©/Data Breach': ['breach', 'leak', 'exposed', 'stolen', 'dump',
                               'å¤–æ´©', 'æ´©æ¼', 'ç«Šå–', 'æ›å…‰'],
        'é§­å®¢æ”»æ“Š/Hacking': ['hack', 'attack', 'intrusion', 'compromise', 'apt',
                           'é§­å®¢', 'æ”»æ“Š', 'å…¥ä¾µ', 'æ»²é€'],
        'ç¶²è·¯é‡£é­š/Phishing': ['phishing', 'scam', 'fraud', 'social engineering',
                            'é‡£é­š', 'è©é¨™', 'ç¤¾äº¤å·¥ç¨‹'],
        'ä¾›æ‡‰éˆ/Supply Chain': ['supply chain', 'software supply', 'dependency',
                               'ä¾›æ‡‰éˆ', 'è»Ÿé«”ä¾›æ‡‰'],
        'é›²ç«¯å®‰å…¨/Cloud Security': ['cloud', 'aws', 'azure', 'gcp', 'kubernetes', 'container',
                                  'é›²ç«¯', 'å®¹å™¨'],
        'ç‰©è¯ç¶²/IoT': ['iot', 'smart device', 'embedded', 'firmware',
                      'ç‰©è¯ç¶²', 'æ™ºæ…§è£ç½®', 'éŸŒé«”'],
        'æ”¿ç­–æ³•è¦/Policy': ['regulation', 'compliance', 'gdpr', 'policy', 'law',
                          'æ³•è¦', 'åˆè¦', 'æ”¿ç­–', 'æ³•å¾‹'],
    }
    
    # é‡è¦é—œéµå­—ï¼ˆç”¨æ–¼æ“·å–ï¼‰
    IMPORTANT_KEYWORDS = [
        'CVE', 'APT', 'zero-day', 'ransomware', 'malware', 'phishing',
        'vulnerability', 'exploit', 'breach', 'attack', 'hack',
        'Microsoft', 'Google', 'Apple', 'Linux', 'Windows', 'Android', 'iOS',
        'FBI', 'NSA', 'CISA', 'Mandiant', 'CrowdStrike',
        'æ¼æ´', 'é§­å®¢', 'æ”»æ“Š', 'å‹’ç´¢', 'æƒ¡æ„ç¨‹å¼', 'è³‡å®‰'
    ]
    
    def __init__(self, custom_sources: Dict[str, str] = None, sources_file: str = None):
        """
        åˆå§‹åŒ–æ”¶é›†å™¨
        
        Args:
            custom_sources: è‡ªè¨‚ä¾†æºå­—å…¸ {åç¨±: RSS URL}
            sources_file: ä¾†æºè¨­å®š JSON æª”æ¡ˆè·¯å¾‘
        """
        self.sources = self.DEFAULT_SOURCES.copy()
        
        # å¾æª”æ¡ˆè¼‰å…¥ä¾†æº
        if sources_file and os.path.exists(sources_file):
            self.load_sources_from_file(sources_file)
        
        # åˆä½µè‡ªè¨‚ä¾†æº
        if custom_sources:
            self.sources.update(custom_sources)
        
        self.news_items: List[NewsItem] = []
        self.df: Optional[pd.DataFrame] = None
        self.seen_hashes: set = set()
    
    def load_sources_from_file(self, filepath: str) -> None:
        """å¾ JSON æª”æ¡ˆè¼‰å…¥ä¾†æºè¨­å®š"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                loaded_sources = json.load(f)
                self.sources.update(loaded_sources)
                print(f"âœ… å·²å¾ {filepath} è¼‰å…¥ {len(loaded_sources)} å€‹ä¾†æº")
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ä¾†æºæª”æ¡ˆå¤±æ•—: {e}")
    
    def save_sources_to_file(self, filepath: str) -> None:
        """å„²å­˜ä¾†æºè¨­å®šåˆ° JSON æª”æ¡ˆ"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.sources, f, ensure_ascii=False, indent=2)
            print(f"âœ… å·²å„²å­˜ {len(self.sources)} å€‹ä¾†æºåˆ° {filepath}")
        except Exception as e:
            print(f"âš ï¸ å„²å­˜ä¾†æºæª”æ¡ˆå¤±æ•—: {e}")
    
    def add_source(self, name: str, url: str) -> None:
        """æ–°å¢ä¾†æº"""
        self.sources[name] = url
        print(f"âœ… å·²æ–°å¢ä¾†æº: {name}")
    
    def remove_source(self, name: str) -> bool:
        """ç§»é™¤ä¾†æº"""
        if name in self.sources:
            del self.sources[name]
            print(f"âœ… å·²ç§»é™¤ä¾†æº: {name}")
            return True
        print(f"âš ï¸ æ‰¾ä¸åˆ°ä¾†æº: {name}")
        return False
    
    def list_sources(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰ä¾†æº"""
        print(f"\nğŸ“° æ–°èä¾†æºæ¸…å–® (å…± {len(self.sources)} å€‹)")
        print("-" * 60)
        for name, url in self.sources.items():
            is_default = "ğŸ“Œ" if name in self.DEFAULT_SOURCES else "â•"
            print(f"{is_default} {name}")
            print(f"   {url}")
        print("-" * 60)
    
    def _generate_hash(self, content: str) -> str:
        """ç”¢ç”Ÿå…§å®¹é›œæ¹Šå€¼ï¼ˆç”¨æ–¼å»é‡ï¼‰"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _clean_html(self, html_content: str) -> str:
        """æ¸…ç† HTML æ¨™ç±¤ï¼Œä¿ç•™ç´”æ–‡å­—"""
        if not html_content:
            return ""
        soup = BeautifulSoup(html_content, 'lxml')
        text = soup.get_text(separator=' ', strip=True)
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _extract_summary(self, entry: dict, max_length: int = 300) -> str:
        """æ“·å–æ‘˜è¦"""
        # å˜—è©¦ä¸åŒçš„æ‘˜è¦æ¬„ä½
        summary = ""
        
        if hasattr(entry, 'summary'):
            summary = entry.summary
        elif hasattr(entry, 'description'):
            summary = entry.description
        elif hasattr(entry, 'content') and entry.content:
            summary = entry.content[0].get('value', '')
        
        # æ¸…ç† HTML
        summary = self._clean_html(summary)
        
        # æˆªæ–·åˆ°æœ€å¤§é•·åº¦
        if len(summary) > max_length:
            summary = summary[:max_length].rsplit(' ', 1)[0] + '...'
        
        return summary
    
    def _parse_date(self, entry: dict) -> str:
        """è§£ææ—¥æœŸ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                dt = datetime(*entry.published_parsed[:6])
                return dt.strftime('%Y-%m-%d %H:%M')
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                dt = datetime(*entry.updated_parsed[:6])
                return dt.strftime('%Y-%m-%d %H:%M')
            elif hasattr(entry, 'published'):
                return entry.published[:16]
        except:
            pass
        return datetime.now().strftime('%Y-%m-%d %H:%M')
    
    def _categorize(self, title: str, summary: str) -> str:
        """è‡ªå‹•åˆ†é¡æ–°è"""
        content = (title + ' ' + summary).lower()
        
        for category, keywords in self.CATEGORY_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in content:
                    return category
        
        return 'å…¶ä»–/Other'
    
    def _extract_keywords(self, title: str, summary: str) -> List[str]:
        """æ“·å–é—œéµå­—"""
        content = title + ' ' + summary
        keywords = []
        
        # æ‰¾ CVE ç·¨è™Ÿ
        cve_pattern = r'CVE-\d{4}-\d+'
        cves = re.findall(cve_pattern, content, re.IGNORECASE)
        keywords.extend([cve.upper() for cve in cves])
        
        # æ‰¾é‡è¦é—œéµå­—
        for kw in self.IMPORTANT_KEYWORDS:
            if kw.lower() in content.lower() and kw not in keywords:
                keywords.append(kw)
        
        return keywords[:10]  # æœ€å¤š 10 å€‹é—œéµå­—
    
    def _fetch_feed(self, source_name: str, url: str) -> List[NewsItem]:
        """æŠ“å–å–®ä¸€ RSS Feed"""
        items = []
        try:
            feed = feedparser.parse(url)
            
            for entry in feed.entries[:20]:  # æ¯å€‹ä¾†æºæœ€å¤š 20 å‰‡
                title = entry.get('title', '').strip()
                link = entry.get('link', '').strip()
                
                if not title or not link:
                    continue
                
                # å»é‡æª¢æŸ¥
                content_hash = self._generate_hash(title + link)
                if content_hash in self.seen_hashes:
                    continue
                self.seen_hashes.add(content_hash)
                
                # æ“·å–è³‡è¨Š
                summary = self._extract_summary(entry)
                date = self._parse_date(entry)
                category = self._categorize(title, summary)
                keywords = self._extract_keywords(title, summary)
                
                item = NewsItem(
                    title=title,
                    link=link,
                    date=date,
                    summary=summary,
                    source=source_name,
                    category=category,
                    keywords=keywords,
                    content_hash=content_hash
                )
                items.append(item)
            
            print(f"âœ… {source_name}: {len(items)} å‰‡æ–°è")
            
        except Exception as e:
            print(f"âŒ {source_name}: æŠ“å–å¤±æ•— - {str(e)[:50]}")
        
        return items
    
    def collect(self, max_workers: int = 5) -> pd.DataFrame:
        """
        æ”¶é›†æ‰€æœ‰ä¾†æºçš„æ–°è
        
        Args:
            max_workers: ä¸¦è¡ŒæŠ“å–çš„æœ€å¤§åŸ·è¡Œç·’æ•¸
            
        Returns:
            åŒ…å«æ‰€æœ‰æ–°èçš„ DataFrame
        """
        print(f"\nğŸ” é–‹å§‹æ”¶é›† {len(self.sources)} å€‹æ–°èä¾†æº...")
        print("-" * 50)
        
        self.news_items = []
        self.seen_hashes = set()
        
        # ä¸¦è¡ŒæŠ“å–
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._fetch_feed, name, url): name 
                for name, url in self.sources.items()
            }
            
            for future in as_completed(futures):
                items = future.result()
                self.news_items.extend(items)
        
        print("-" * 50)
        print(f"ğŸ“Š å…±æ”¶é›† {len(self.news_items)} å‰‡ä¸é‡è¤‡æ–°è")
        
        # è½‰æ›ç‚º DataFrame
        if self.news_items:
            self.df = pd.DataFrame([asdict(item) for item in self.news_items])
            self.df['keywords_str'] = self.df['keywords'].apply(
                lambda x: ', '.join(x) if x else ''
            )
            # ä¾æ—¥æœŸæ’åº
            self.df = self.df.sort_values('date', ascending=False).reset_index(drop=True)
        else:
            self.df = pd.DataFrame()
        
        return self.df
    
    def search(self, query: str) -> pd.DataFrame:
        """
        æœå°‹æ–°è
        
        Args:
            query: æœå°‹é—œéµå­—
            
        Returns:
            ç¬¦åˆçš„æ–°è DataFrame
        """
        if self.df is None or self.df.empty:
            return pd.DataFrame()
        
        query_lower = query.lower()
        mask = (
            self.df['title'].str.lower().str.contains(query_lower, na=False) |
            self.df['summary'].str.lower().str.contains(query_lower, na=False) |
            self.df['keywords_str'].str.lower().str.contains(query_lower, na=False)
        )
        return self.df[mask].reset_index(drop=True)
    
    def filter_by_category(self, category: str) -> pd.DataFrame:
        """ä¾åˆ†é¡ç¯©é¸"""
        if self.df is None or self.df.empty:
            return pd.DataFrame()
        
        mask = self.df['category'].str.contains(category, case=False, na=False)
        return self.df[mask].reset_index(drop=True)
    
    def filter_by_source(self, source: str) -> pd.DataFrame:
        """ä¾ä¾†æºç¯©é¸"""
        if self.df is None or self.df.empty:
            return pd.DataFrame()
        
        mask = self.df['source'].str.contains(source, case=False, na=False)
        return self.df[mask].reset_index(drop=True)
    
    def filter_by_date(self, start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """ä¾æ—¥æœŸç¯„åœç¯©é¸"""
        if self.df is None or self.df.empty:
            return pd.DataFrame()
        
        result = self.df.copy()
        
        if start_date:
            result = result[result['date'] >= start_date]
        if end_date:
            result = result[result['date'] <= end_date]
        
        return result.reset_index(drop=True)
    
    def get_summary_stats(self) -> Dict:
        """å–å¾—çµ±è¨ˆæ‘˜è¦"""
        if self.df is None or self.df.empty:
            return {}
        
        return {
            'total_news': len(self.df),
            'sources': self.df['source'].value_counts().to_dict(),
            'categories': self.df['category'].value_counts().to_dict(),
            'date_range': {
                'earliest': self.df['date'].min(),
                'latest': self.df['date'].max()
            }
        }
    
    def to_excel(self, filepath: str = 'security_news_report.xlsx') -> None:
        """
        åŒ¯å‡º Excel å ±è¡¨ï¼ˆå«æ ¼å¼åŒ–ï¼‰
        
        Args:
            filepath: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        if self.df is None or self.df.empty:
            print("âš ï¸ æ²’æœ‰è³‡æ–™å¯åŒ¯å‡º")
            return
        
        from openpyxl import Workbook
        from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
        from openpyxl.utils.dataframe import dataframe_to_rows
        
        wb = Workbook()
        
        # ===== å·¥ä½œè¡¨ 1: æ–°èæ¸…å–® =====
        ws1 = wb.active
        ws1.title = "è³‡å®‰æ–°è"
        
        # æ¨™é¡Œåˆ—
        headers = ['åºè™Ÿ', 'æ—¥æœŸ', 'ä¾†æº', 'åˆ†é¡', 'æ¨™é¡Œ', 'æ‘˜è¦', 'é—œéµå­—', 'é€£çµ']
        header_fill = PatternFill(start_color="1F4E79", end_color="1F4E79", fill_type="solid")
        header_font = Font(color="FFFFFF", bold=True, size=11)
        
        for col, header in enumerate(headers, 1):
            cell = ws1.cell(row=1, column=col, value=header)
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center')
        
        # åˆ†é¡é¡è‰²
        category_colors = {
            'æƒ¡æ„ç¨‹å¼': 'FFE0E0',
            'æ¼æ´': 'FFF3E0',
            'è³‡æ–™å¤–æ´©': 'F3E5F5',
            'é§­å®¢æ”»æ“Š': 'E3F2FD',
            'ç¶²è·¯é‡£é­š': 'FFF8E1',
            'ä¾›æ‡‰éˆ': 'E8F5E9',
            'é›²ç«¯å®‰å…¨': 'E0F7FA',
            'ç‰©è¯ç¶²': 'FBE9E7',
            'æ”¿ç­–æ³•è¦': 'ECEFF1',
        }
        
        # è³‡æ–™åˆ—
        thin_border = Border(
            left=Side(style='thin', color='CCCCCC'),
            right=Side(style='thin', color='CCCCCC'),
            top=Side(style='thin', color='CCCCCC'),
            bottom=Side(style='thin', color='CCCCCC')
        )
        
        for idx, row in self.df.iterrows():
            row_num = idx + 2
            
            ws1.cell(row=row_num, column=1, value=idx + 1)
            ws1.cell(row=row_num, column=2, value=row['date'])
            ws1.cell(row=row_num, column=3, value=row['source'])
            
            # åˆ†é¡ï¼ˆå¸¶é¡è‰²ï¼‰
            cat_cell = ws1.cell(row=row_num, column=4, value=row['category'])
            for cat_key, color in category_colors.items():
                if cat_key in row['category']:
                    cat_cell.fill = PatternFill(start_color=color, end_color=color, fill_type="solid")
                    break
            
            ws1.cell(row=row_num, column=5, value=row['title'])
            ws1.cell(row=row_num, column=6, value=row['summary'])
            ws1.cell(row=row_num, column=7, value=row['keywords_str'])
            
            # é€£çµ
            link_cell = ws1.cell(row=row_num, column=8, value=row['link'])
            link_cell.hyperlink = row['link']
            link_cell.font = Font(color="0563C1", underline="single")
            
            # å¥—ç”¨æ¡†ç·š
            for col in range(1, 9):
                ws1.cell(row=row_num, column=col).border = thin_border
                ws1.cell(row=row_num, column=col).alignment = Alignment(
                    wrap_text=True, vertical='top'
                )
        
        # èª¿æ•´æ¬„å¯¬
        ws1.column_dimensions['A'].width = 6
        ws1.column_dimensions['B'].width = 16
        ws1.column_dimensions['C'].width = 18
        ws1.column_dimensions['D'].width = 20
        ws1.column_dimensions['E'].width = 50
        ws1.column_dimensions['F'].width = 60
        ws1.column_dimensions['G'].width = 30
        ws1.column_dimensions['H'].width = 40
        
        # å‡çµé¦–åˆ—
        ws1.freeze_panes = 'A2'
        
        # ===== å·¥ä½œè¡¨ 2: çµ±è¨ˆåˆ†æ =====
        ws2 = wb.create_sheet(title="çµ±è¨ˆåˆ†æ")
        
        stats = self.get_summary_stats()
        
        # æ¨™é¡Œ
        ws2.cell(row=1, column=1, value="ğŸ“Š è³‡å®‰æ–°èçµ±è¨ˆå ±å‘Š")
        ws2.cell(row=1, column=1).font = Font(bold=True, size=14)
        ws2.merge_cells('A1:C1')
        
        # ç¸½è¦½
        ws2.cell(row=3, column=1, value="ç¸½è¦½")
        ws2.cell(row=3, column=1).font = Font(bold=True)
        ws2.cell(row=4, column=1, value="æ–°èç¸½æ•¸")
        ws2.cell(row=4, column=2, value=stats.get('total_news', 0))
        ws2.cell(row=5, column=1, value="æœ€æ—©æ—¥æœŸ")
        ws2.cell(row=5, column=2, value=stats.get('date_range', {}).get('earliest', ''))
        ws2.cell(row=6, column=1, value="æœ€æ–°æ—¥æœŸ")
        ws2.cell(row=6, column=2, value=stats.get('date_range', {}).get('latest', ''))
        
        # ä¾†æºçµ±è¨ˆ
        ws2.cell(row=8, column=1, value="ä¾†æºåˆ†å¸ƒ")
        ws2.cell(row=8, column=1).font = Font(bold=True)
        row_num = 9
        for source, count in stats.get('sources', {}).items():
            ws2.cell(row=row_num, column=1, value=source)
            ws2.cell(row=row_num, column=2, value=count)
            row_num += 1
        
        # åˆ†é¡çµ±è¨ˆ
        row_num += 1
        ws2.cell(row=row_num, column=1, value="åˆ†é¡åˆ†å¸ƒ")
        ws2.cell(row=row_num, column=1).font = Font(bold=True)
        row_num += 1
        for category, count in stats.get('categories', {}).items():
            ws2.cell(row=row_num, column=1, value=category)
            ws2.cell(row=row_num, column=2, value=count)
            row_num += 1
        
        ws2.column_dimensions['A'].width = 25
        ws2.column_dimensions['B'].width = 15
        
        # å„²å­˜
        wb.save(filepath)
        print(f"ğŸ“Š Excel å ±è¡¨å·²åŒ¯å‡ºè‡³ {filepath}")
    
    def to_csv(self, filepath: str = 'security_news.csv') -> None:
        """åŒ¯å‡º CSV"""
        if self.df is None or self.df.empty:
            print("âš ï¸ æ²’æœ‰è³‡æ–™å¯åŒ¯å‡º")
            return
        
        export_df = self.df[['date', 'source', 'category', 'title', 'summary', 'keywords_str', 'link']]
        export_df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ å·²åŒ¯å‡ºè‡³ {filepath}")
    
    def to_json(self, filepath: str = 'security_news.json') -> None:
        """åŒ¯å‡º JSON"""
        if self.df is None or self.df.empty:
            print("âš ï¸ æ²’æœ‰è³‡æ–™å¯åŒ¯å‡º")
            return
        
        data = {
            'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_count': len(self.df),
            'sources': list(self.df['source'].unique()),
            'categories': list(self.df['category'].unique()),
            'news': self.df.to_dict(orient='records')
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å·²åŒ¯å‡ºè‡³ {filepath}")


def load_demo_data() -> List[Dict]:
    """è¼‰å…¥å±•ç¤ºè³‡æ–™ï¼ˆç•¶ç„¡æ³•é€£ç·šæ™‚ä½¿ç”¨ï¼‰"""
    return [
        {
            "title": "å¾®è»Ÿç™¼å¸ƒç·Šæ€¥æ›´æ–°ä¿®è£œ WSUS é‡å¤§æ¼æ´ CVE-2025-59287",
            "link": "https://example.com/news/1",
            "date": "2025-11-24 14:30",
            "summary": "å¾®è»Ÿç·Šæ€¥ç™¼å¸ƒæ›´æ–°ä¿®è£œ Windows Server Update Services (WSUS) ä¸­çš„é‡å¤§é ç«¯ç¨‹å¼ç¢¼åŸ·è¡Œæ¼æ´ã€‚è©²æ¼æ´å…è¨±æœªç¶“é©—è­‰çš„æ”»æ“Šè€…åœ¨ç›®æ¨™ç³»çµ±ä¸ŠåŸ·è¡Œä»»æ„ç¨‹å¼ç¢¼ã€‚",
            "source": "The Hacker News",
            "category": "æ¼æ´/Vulnerability",
            "keywords": ["CVE-2025-59287", "Microsoft", "WSUS", "vulnerability"]
        },
        {
            "title": "CrazyHunter å‹’ç´¢è»Ÿé«”é‡å°å°ç£é‡‘èæ¥­ç™¼å‹•æ”»æ“Š",
            "link": "https://example.com/news/2",
            "date": "2025-11-23 10:15",
            "summary": "è³‡å®‰ç ”ç©¶äººå“¡ç™¼ç¾æ–°å‹å‹’ç´¢è»Ÿé«” CrazyHunter æ­£é‡å°å°ç£é‡‘èæ©Ÿæ§‹ç™¼å‹•æ”»æ“Šï¼Œè©²æƒ¡æ„ç¨‹å¼ä½¿ç”¨é›™é‡å‹’ç´¢ç­–ç•¥ï¼Œé™¤åŠ å¯†æª”æ¡ˆå¤–é‚„å¨è„…å…¬é–‹æ•æ„Ÿè³‡æ–™ã€‚",
            "source": "iThome è³‡å®‰",
            "category": "æƒ¡æ„ç¨‹å¼/Malware",
            "keywords": ["ransomware", "CrazyHunter", "å‹’ç´¢è»Ÿé«”", "é‡‘èæ¥­"]
        },
        {
            "title": "ç ”ç©¶äººå“¡ç™¼ç¾ AI æ¨¡å‹å¯è¢«åˆ©ç”¨ç”¢ç”Ÿæƒ¡æ„ç¨‹å¼ç¢¼",
            "link": "https://example.com/news/3",
            "date": "2025-11-22 16:45",
            "summary": "è³‡å®‰ç ”ç©¶åœ˜éšŠç™¼è¡¨å ±å‘ŠæŒ‡å‡ºï¼Œå¤§å‹èªè¨€æ¨¡å‹å¯èƒ½è¢«æƒ¡æ„è¡Œç‚ºè€…åˆ©ç”¨ä¾†ç”¢ç”Ÿæƒ¡æ„ç¨‹å¼ç¢¼ï¼Œå‘¼ç±² AI é–‹ç™¼å•†åŠ å¼·å®‰å…¨é˜²è­·æªæ–½ã€‚",
            "source": "SecurityWeek",
            "category": "å…¶ä»–/Other",
            "keywords": ["AI", "malware", "ç ”ç©¶"]
        },
        {
            "title": "Oracle è³‡æ–™åº«ä¼ºæœå™¨ç™¼ç¾é«˜é¢¨éšª SQL æ³¨å…¥æ¼æ´",
            "link": "https://example.com/news/4",
            "date": "2025-11-21 09:20",
            "summary": "Oracle ç™¼å¸ƒå®‰å…¨å…¬å‘Šï¼Œé‡å°å…¶è³‡æ–™åº«ä¼ºæœå™¨ç”¢å“ä¸­çš„ SQL æ³¨å…¥æ¼æ´ CVE-2025-61757 æä¾›ä¿®è£œç¨‹å¼ï¼Œè©²æ¼æ´ CVSS è©•åˆ†é«˜é” 9.8ã€‚",
            "source": "Dark Reading",
            "category": "æ¼æ´/Vulnerability",
            "keywords": ["CVE-2025-61757", "Oracle", "SQL injection", "vulnerability"]
        },
        {
            "title": "ä¸­åœ‹é§­å®¢çµ„ç¹” APT24 é‡å°æ±å—äºæ”¿åºœæ©Ÿé—œç™¼å‹•æ”»æ“Š",
            "link": "https://example.com/news/5",
            "date": "2025-11-20 11:30",
            "summary": "å¨è„…æƒ…å ±å…¬å¸æ­éœ²ä¸­åœ‹åœ‹å®¶ç´šé§­å®¢çµ„ç¹” APT24 è¿‘æœŸé‡å°æ±å—äºå¤šåœ‹æ”¿åºœæ©Ÿé—œç™¼å‹•ç¶²è·¯é–“è«œæ´»å‹•ï¼Œç«Šå–æ•æ„Ÿå¤–äº¤æ–‡ä»¶ã€‚",
            "source": "The Hacker News",
            "category": "é§­å®¢æ”»æ“Š/Hacking",
            "keywords": ["APT24", "China", "é§­å®¢", "æ”¿åºœ"]
        },
        {
            "title": "å°ç£ä¼æ¥­é­ä¾›æ‡‰éˆæ”»æ“Šï¼Œæ•¸ç™¾å®¶å…¬å¸å—å½±éŸ¿",
            "link": "https://example.com/news/6",
            "date": "2025-11-19 15:00",
            "summary": "èª¿æŸ¥å±€è³‡å®‰å·¥ä½œç«™è­¦å‘Šï¼Œé§­å®¢é€éå…¥ä¾µæœ¬åœŸè»Ÿé«”ä¾›æ‡‰å•†ï¼Œæ¤å…¥æƒ¡æ„ç¨‹å¼å¾Œé–€ï¼Œå½±éŸ¿ä½¿ç”¨è©²è»Ÿé«”çš„æ•¸ç™¾å®¶å°ç£ä¼æ¥­ã€‚",
            "source": "TWCERT/CC",
            "category": "ä¾›æ‡‰éˆ/Supply Chain",
            "keywords": ["supply chain", "ä¾›æ‡‰éˆ", "å°ç£", "malware"]
        },
        {
            "title": "AWS S3 å„²å­˜æ¡¶é…ç½®éŒ¯èª¤å°è‡´ç™¾è¬ç”¨æˆ¶è³‡æ–™å¤–æ´©",
            "link": "https://example.com/news/7",
            "date": "2025-11-18 13:45",
            "summary": "æŸçŸ¥å SaaS å¹³å°å›  AWS S3 å„²å­˜æ¡¶é…ç½®ä¸ç•¶ï¼Œå°è‡´è¶…é 100 è¬ç”¨æˆ¶å€‹è³‡å¤–æ´©ï¼ŒåŒ…æ‹¬å§“åã€é›»å­éƒµä»¶åŠåŠ å¯†å¯†ç¢¼ã€‚",
            "source": "BleepingComputer",
            "category": "è³‡æ–™å¤–æ´©/Data Breach",
            "keywords": ["AWS", "S3", "data breach", "é›²ç«¯å®‰å…¨"]
        },
        {
            "title": "æ–°å‹ç¶²è·¯é‡£é­šæ”»æ“Šå½è£æˆ Microsoft 365 ç™»å…¥é é¢",
            "link": "https://example.com/news/8",
            "date": "2025-11-17 10:00",
            "summary": "è³‡å®‰å…¬å¸ç™¼ç¾æ–°å‹é‡£é­šæ”»æ“Šæ´»å‹•ï¼Œé§­å®¢å»ºç«‹é«˜åº¦ä»¿çœŸçš„ Microsoft 365 ç™»å…¥é é¢ï¼Œå·²æœ‰å¤šå®¶ä¼æ¥­å“¡å·¥å—å®³ã€‚",
            "source": "Krebs on Security",
            "category": "ç¶²è·¯é‡£é­š/Phishing",
            "keywords": ["phishing", "Microsoft 365", "é‡£é­š"]
        },
        {
            "title": "æ™ºæ…§å®¶é›»æ¼æ´å…è¨±é§­å®¢é ç«¯æ§åˆ¶è¨­å‚™",
            "link": "https://example.com/news/9",
            "date": "2025-11-16 14:20",
            "summary": "ç ”ç©¶äººå“¡åœ¨å¤šæ¬¾æ™ºæ…§å®¶é›»ä¸­ç™¼ç¾é‡å¤§å®‰å…¨æ¼æ´ï¼Œæ”»æ“Šè€…å¯åˆ©ç”¨é€™äº›æ¼æ´é ç«¯æ§åˆ¶è¨­å‚™ï¼Œç”šè‡³ç›£è½ç”¨æˆ¶å°è©±ã€‚",
            "source": "HackRead",
            "category": "ç‰©è¯ç¶²/IoT",
            "keywords": ["IoT", "ç‰©è¯ç¶²", "smart home", "vulnerability"]
        },
        {
            "title": "æ­ç›Ÿé€šéæ–°ç¶²è·¯å®‰å…¨æ³•è¦å¼·åŒ–é—œéµåŸºç¤è¨­æ–½ä¿è­·",
            "link": "https://example.com/news/10",
            "date": "2025-11-15 09:00",
            "summary": "æ­ç›Ÿè­°æœƒé€šéæ–°ç‰ˆç¶²è·¯å®‰å…¨æ³•è¦ï¼Œè¦æ±‚é—œéµåŸºç¤è¨­æ–½ç‡Ÿé‹å•†å¼·åŒ–è³‡å®‰æªæ–½ï¼Œé•è€…å°‡é¢è‡¨é«˜é¡ç½°æ¬¾ã€‚",
            "source": "SecurityWeek",
            "category": "æ”¿ç­–æ³•è¦/Policy",
            "keywords": ["EU", "æ³•è¦", "policy", "é—œéµåŸºç¤è¨­æ–½"]
        },
        {
            "title": "LockBit å‹’ç´¢è»Ÿé«”é›†åœ˜å®£ç¨±å…¥ä¾µå¤šåœ‹é†«ç™‚æ©Ÿæ§‹",
            "link": "https://example.com/news/11",
            "date": "2025-11-14 16:30",
            "summary": "æƒ¡åæ˜­å½°çš„ LockBit å‹’ç´¢è»Ÿé«”é›†åœ˜åœ¨æš—ç¶²å…¬å¸ƒå¤šå®¶é†«ç™‚æ©Ÿæ§‹è³‡æ–™ï¼Œè¦æ±‚æ”¯ä»˜è´–é‡‘å¦å‰‡å°‡å…¬é–‹ç—…æ‚£å€‹è³‡ã€‚",
            "source": "TWCERT/CC",
            "category": "æƒ¡æ„ç¨‹å¼/Malware",
            "keywords": ["LockBit", "ransomware", "å‹’ç´¢è»Ÿé«”", "é†«ç™‚"]
        },
        {
            "title": "Grafana ç™¼å¸ƒé‡å¤§å®‰å…¨æ›´æ–°ä¿®è£œèªè­‰ç¹éæ¼æ´",
            "link": "https://example.com/news/12",
            "date": "2025-11-14 08:45",
            "summary": "é–‹æºç›£æ§å¹³å° Grafana ç™¼å¸ƒç·Šæ€¥å®‰å…¨æ›´æ–°ï¼Œä¿®è£œ CVE-2025-41115 èªè­‰ç¹éæ¼æ´ï¼Œå»ºè­°ç”¨æˆ¶ç«‹å³å‡ç´šã€‚",
            "source": "iThome è³‡å®‰",
            "category": "æ¼æ´/Vulnerability",
            "keywords": ["CVE-2025-41115", "Grafana", "vulnerability", "èªè­‰ç¹é"]
        }
    ]


# ä¸»ç¨‹å¼ï¼ˆå‘½ä»¤åˆ—ä½¿ç”¨ï¼‰
if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ›¡ï¸  è³‡å®‰æ–°èæ”¶é›†å™¨")
    print("=" * 60)
    
    collector = SecurityNewsCollector()
    df = collector.collect()
    
    if df.empty:
        print("\nâš ï¸ ç„¡æ³•é€£ç·šè‡³æ–°èä¾†æºï¼Œè¼‰å…¥å±•ç¤ºè³‡æ–™...")
        demo_data = load_demo_data()
        
        for item in demo_data:
            news_item = NewsItem(
                title=item['title'],
                link=item['link'],
                date=item['date'],
                summary=item['summary'],
                source=item['source'],
                category=item['category'],
                keywords=item['keywords'],
                content_hash=collector._generate_hash(item['title'] + item['link'])
            )
            collector.news_items.append(news_item)
        
        collector.df = pd.DataFrame([asdict(item) for item in collector.news_items])
        collector.df['keywords_str'] = collector.df['keywords'].apply(
            lambda x: ', '.join(x) if x else ''
        )
        collector.df = collector.df.sort_values('date', ascending=False).reset_index(drop=True)
        print(f"ğŸ“Š å·²è¼‰å…¥ {len(collector.df)} å‰‡å±•ç¤ºæ–°è")
    
    # åŒ¯å‡ºæª”æ¡ˆ
    os.makedirs('site', exist_ok=True)
    collector.to_json('site/data.json')
    collector.to_excel('site/security_news_report.xlsx')
    collector.to_csv('site/security_news.csv')
    
    # é¡¯ç¤ºçµ±è¨ˆ
    stats = collector.get_summary_stats()
    print(f"\nğŸ“ˆ çµ±è¨ˆæ‘˜è¦:")
    print(f"   æ–°èç¸½æ•¸: {stats.get('total_news', 0)}")
    print(f"   ä¾†æºæ•¸: {len(stats.get('sources', {}))}")
    print(f"   åˆ†é¡æ•¸: {len(stats.get('categories', {}))}")
